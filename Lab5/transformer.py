import torch
import torch.nn as nn

class ScaledDotProductAttention(nn.Module):
    pass

class PositionalEncoding(nn.Module):
    pass

class FeedForward(nn.Module):
    pass

class TransformerEncoderLayer(nn.Module):
    pass

class TransformerDecoderLayer(nn.Module):
    pass

def generate_padding_mask() -> torch.Tensor:
    pass

def generate_sequential_mask() -> torch.BoolTensor:
    pass